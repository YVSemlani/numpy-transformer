# Transformer in Numpy

## To-Do:

- Check whether cross attention is being done correctly (Q is encoder output and KV is decoder input)


## Project Structure


## Errors Encountered

- Cross attention used the wrong values for Q, K, and V
- Gradient flow was incorrect (several fixes had to be made here)